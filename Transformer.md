___
# TRANSFORMER
Links: [[Deep Learning]]
Status: #ðŸŒž 
Tags: [[Natural Language Processing (NLP)]] [[ChatGPT]]

<!--- Created on: 2023.08.28, 18:32 --->

- deep learning architecture introduced in the paper [[Attention is All You Need]]
- uses [[Self-Attention]] mechanisms to process input data in parallel, enabling efficient encoding of sequential and contextual information
- are not models themselves but rather a type of architecture
___

# References
## Paper
- [[Attention Is All You Need]]
- [[DETR]]
- [TrackFormer: Multi-Object Tracking with Transformers](https://arxiv.org/pdf/2101.02702.pdf)
- [Transformers for Multi-Object Tracking on Point Clouds](https://arxiv.org/pdf/2205.15730.pdf)
- [MOTR: End-to-End Multiple-Object Tracking with Transformer](https://arxiv.org/pdf/2105.03247.pdf)

## Websites
- [Transformers from Scratch by Peter Bloem](https://peterbloem.nl/blog/transformers)
- [The Illustrated Transformer by Jay Allamar](https://jalammar.github.io/illustrated-transformer/)
- [Attention? Attention! by Lilian Weng](https://lilianweng.github.io/posts/2018-06-24-attention/)

## Videos
- [Facebook DETR | ML Coding Series | End to end object detection with transformers](https://www.youtube.com/watch?v=xkuoZ50gK4Q&list=PLBoQnSflObckGnAS9mXjqCZhg7VTz4x8n&index=17)
- [DETR: End-to-End Object Detection with Transformers (Paper Explained)](https://www.youtube.com/watch?v=T35ba_VXkMY)
- [Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?si=hQBldif7FjEj54XO)